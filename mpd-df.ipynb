{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "acfb0247",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: scipy in d:\\anaconda3\\envs\\opendriver\\lib\\site-packages (1.17.0)\n",
      "Requirement already satisfied: numpy<2.7,>=1.26.4 in d:\\anaconda3\\envs\\opendriver\\lib\\site-packages (from scipy) (2.3.5)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install scipy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "87758f3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['__header__', '__version__', '__globals__', 'ecg_data', 'annotation'])\n",
      "(7202, 1, 1024)\n",
      "<class 'numpy.ndarray'>\n",
      "[[[ 3.81475547e-08 -1.48775463e-06 -5.76028077e-06 ...  4.31067369e-06\n",
      "   -2.93736172e-06 -6.98100252e-06]]\n",
      "\n",
      " [[-4.76844434e-06 -3.01365682e-06 -6.21805142e-06 ...  6.70366979e-04\n",
      "    7.09735256e-04  7.57419699e-04]]\n",
      "\n",
      " [[ 8.17845426e-04  8.83764401e-04  9.45944915e-04 ... -1.00328069e-05\n",
      "   -1.37712673e-05 -1.36949722e-05]]\n",
      "\n",
      " [[-9.26985580e-06 -8.58319982e-06 -1.44579232e-05 ...  4.20004578e-05\n",
      "    3.41420615e-05  3.36079957e-05]]\n",
      "\n",
      " [[ 4.08560311e-05  4.72648203e-05  4.92484932e-05 ...  5.61150530e-05\n",
      "    4.46707866e-05  3.85671778e-05]]]\n"
     ]
    }
   ],
   "source": [
    "import scipy.io as sio\n",
    "import numpy as np\n",
    "\n",
    "# Load a .mat file\n",
    "mat_data = sio.loadmat('Preprocessed-Dataset-with-Integrated-Annotations/01/MPDDF_preprocessed_01_ECG.mat')\n",
    "\n",
    "# View what's in the file\n",
    "print(mat_data.keys())\n",
    "\n",
    "# Access a specific variable\n",
    "data = mat_data['ecg_data']\n",
    "\n",
    "# Work with the data\n",
    "print(data.shape)\n",
    "print(type(data))\n",
    "print(data[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "44782ac8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File 01 - Label distribution: [(np.int64(0), np.int64(6556)), (np.int64(1), np.int64(646))]\n",
      "File 02 - Label distribution: [(np.int64(0), np.int64(4849)), (np.int64(1), np.int64(1931)), (np.int64(2), np.int64(422))]\n",
      "File 03 - Label distribution: [(np.int64(0), np.int64(6386)), (np.int64(1), np.int64(818))]\n",
      "File 04 - Label distribution: [(np.int64(0), np.int64(2246)), (np.int64(1), np.int64(2388)), (np.int64(2), np.int64(2403)), (np.int64(3), np.int64(157))]\n",
      "File 05 - Label distribution: [(np.int64(0), np.int64(1374)), (np.int64(1), np.int64(1884)), (np.int64(9), np.int64(3943))]\n",
      "File 06 - Label distribution: [(np.int64(0), np.int64(5678)), (np.int64(1), np.int64(1464)), (np.int64(2), np.int64(60))]\n",
      "File 07 - Label distribution: [(np.int64(0), np.int64(6819)), (np.int64(1), np.int64(213)), (np.int64(2), np.int64(169))]\n",
      "File 08 - Label distribution: [(np.int64(0), np.int64(7175)), (np.int64(1), np.int64(21))]\n",
      "File 09 - Label distribution: [(np.int64(0), np.int64(1892)), (np.int64(1), np.int64(4094)), (np.int64(2), np.int64(1209))]\n",
      "File 10 - Label distribution: [(np.int64(0), np.int64(5964)), (np.int64(1), np.int64(1134)), (np.int64(2), np.int64(327)), (np.int64(3), np.int64(366)), (np.int64(4), np.int64(310))]\n",
      "File 11 - Label distribution: [(np.int64(0), np.int64(5915)), (np.int64(1), np.int64(1219)), (np.int64(2), np.int64(107))]\n",
      "File 12 - Label distribution: [(np.int64(0), np.int64(6907)), (np.int64(1), np.int64(41)), (np.int64(2), np.int64(283))]\n",
      "File 13 - Label distribution: [(np.int64(0), np.int64(4325)), (np.int64(1), np.int64(3625)), (np.int64(2), np.int64(146))]\n",
      "File 14 - Label distribution: [(np.int64(0), np.int64(6954)), (np.int64(1), np.int64(302))]\n",
      "File 15 - Label distribution: [(np.int64(0), np.int64(4392)), (np.int64(1), np.int64(3107)), (np.int64(2), np.int64(301))]\n",
      "File 16 - Label distribution: [(np.int64(0), np.int64(7063)), (np.int64(1), np.int64(438))]\n",
      "File 17 - Label distribution: [(np.int64(0), np.int64(4293)), (np.int64(1), np.int64(2452)), (np.int64(2), np.int64(606))]\n",
      "File 18 - Label distribution: [(np.int64(0), np.int64(7140)), (np.int64(1), np.int64(92)), (np.int64(2), np.int64(149))]\n",
      "File 19 - Label distribution: [(np.int64(0), np.int64(6675)), (np.int64(1), np.int64(111)), (np.int64(2), np.int64(340)), (np.int64(3), np.int64(256))]\n",
      "File 20 - Label distribution: [(np.int64(0), np.int64(7226)), (np.int64(1), np.int64(35))]\n",
      "File 21 - Label distribution: [(np.int64(0), np.int64(7059)), (np.int64(2), np.int64(236)), (np.int64(9), np.int64(206))]\n",
      "File 22 - Label distribution: [(np.int64(1), np.int64(1903)), (np.int64(2), np.int64(5898))]\n",
      "File 23 - Label distribution: [(np.int64(0), np.int64(7320)), (np.int64(1), np.int64(210))]\n",
      "File 24 - Label distribution: [(np.int64(0), np.int64(7291)), (np.int64(1), np.int64(221)), (np.int64(2), np.int64(30))]\n",
      "File 25 - Label distribution: [(np.int64(0), np.int64(3620)), (np.int64(1), np.int64(3396)), (np.int64(2), np.int64(476))]\n",
      "File 26 - Label distribution: [(np.int64(0), np.int64(6688)), (np.int64(1), np.int64(534)), (np.int64(2), np.int64(273))]\n",
      "File 27 - Label distribution: [(np.int64(0), np.int64(7588)), (np.int64(1), np.int64(61)), (np.int64(2), np.int64(152))]\n",
      "File 28 - Label distribution: [(np.int64(0), np.int64(7392)), (np.int64(2), np.int64(229))]\n",
      "File 29 - Label distribution: [(np.int64(0), np.int64(7469)), (np.int64(1), np.int64(194)), (np.int64(2), np.int64(131))]\n",
      "File 30 - Label distribution: [(np.int64(0), np.int64(5140)), (np.int64(1), np.int64(887)), (np.int64(2), np.int64(1116)), (np.int64(8), np.int64(593))]\n",
      "File 31 - Label distribution: [(np.int64(0), np.int64(4129)), (np.int64(1), np.int64(2775)), (np.int64(2), np.int64(597))]\n",
      "File 32 - Label distribution: [(np.int64(0), np.int64(3509)), (np.int64(1), np.int64(3806)), (np.int64(2), np.int64(187))]\n",
      "File 33 - Label distribution: [(np.int64(0), np.int64(5770)), (np.int64(1), np.int64(1458)), (np.int64(2), np.int64(273))]\n",
      "File 34 - Label distribution: [(np.int64(0), np.int64(6380)), (np.int64(1), np.int64(684)), (np.int64(2), np.int64(194)), (np.int64(3), np.int64(242))]\n",
      "File 35 - Label distribution: [(np.int64(0), np.int64(2350)), (np.int64(1), np.int64(4775)), (np.int64(2), np.int64(415)), (np.int64(3), np.int64(21))]\n",
      "File 36 - Label distribution: [(np.int64(0), np.int64(5898)), (np.int64(1), np.int64(1452)), (np.int64(2), np.int64(148))]\n",
      "File 37 - Label distribution: [(np.int64(0), np.int64(6925)), (np.int64(1), np.int64(400)), (np.int64(2), np.int64(178))]\n",
      "File 38 - Label distribution: [(np.int64(0), np.int64(7606)), (np.int64(2), np.int64(200))]\n",
      "File 39 - Label distribution: [(np.int64(0), np.int64(7140)), (np.int64(1), np.int64(177)), (np.int64(2), np.int64(213))]\n",
      "File 40 - Label distribution: [(np.int64(0), np.int64(4271)), (np.int64(1), np.int64(2871)), (np.int64(2), np.int64(424))]\n",
      "File 41 - Label distribution: [(np.int64(0), np.int64(5377)), (np.int64(1), np.int64(1937)), (np.int64(2), np.int64(217))]\n",
      "File 42 - Label distribution: [(np.int64(0), np.int64(5374))]\n",
      "File 43 - Label distribution: [(np.int64(0), np.int64(4520)), (np.int64(1), np.int64(2334)), (np.int64(2), np.int64(648))]\n",
      "File 44 - Label distribution: [(np.int64(0), np.int64(5983)), (np.int64(1), np.int64(1282)), (np.int64(2), np.int64(267))]\n",
      "File 45 - Label distribution: [(np.int64(0), np.int64(5012)), (np.int64(1), np.int64(1849)), (np.int64(2), np.int64(210)), (np.int64(3), np.int64(456)), (np.int64(9), np.int64(35))]\n",
      "File 46 - Label distribution: [(np.int64(0), np.int64(5148)), (np.int64(1), np.int64(2179)), (np.int64(2), np.int64(174))]\n",
      "File 47 - Label distribution: [(np.int64(0), np.int64(3766)), (np.int64(1), np.int64(261)), (np.int64(2), np.int64(176)), (np.int64(9), np.int64(3300))]\n",
      "File 48 - Label distribution: [(np.int64(0), np.int64(5832)), (np.int64(1), np.int64(1505)), (np.int64(2), np.int64(211))]\n",
      "File 49 - Label distribution: [(np.int64(0), np.int64(7342))]\n",
      "File 50 - Label distribution: [(np.int64(0), np.int64(6817)), (np.int64(1), np.int64(835)), (np.int64(2), np.int64(178))]\n",
      "[np.int64(64001), np.int64(19973), np.int64(1498), np.int64(310), 0, 0, 0, np.int64(593), np.int64(7484)]\n"
     ]
    }
   ],
   "source": [
    "import scipy.io as sio\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Path to your ECG file\n",
    "mat_path = \"Preprocessed-Dataset-with-Integrated-Annotations/31/MPDDF_preprocessed_31_ECG.mat\"\n",
    "m_path = \"Preprocessed-Dataset-with-Integrated-Annotations\"\n",
    "# Load .mat file\n",
    "label1=0\n",
    "label2=0\n",
    "label3=0\n",
    "label4=0\n",
    "label5=0\n",
    "label6=0\n",
    "label7=0\n",
    "label8=0\n",
    "label9=0\n",
    "for i in range(1,51):\n",
    "    mat_path = f\"{m_path}/{i:02d}/MPDDF_preprocessed_{i:02d}_ECG.mat\"\n",
    "    data = sio.loadmat(mat_path)\n",
    "\n",
    "    # See available keys\n",
    "    # print(\"Keys in .mat file:\")\n",
    "    # print(data.keys())\n",
    "\n",
    "    # Extract annotation\n",
    "    annotation = data['annotation']   # usually shape (1, N)\n",
    "\n",
    "    # print(\"\\nRaw annotation shape:\", annotation.shape)\n",
    "\n",
    "    # Flatten to 1D\n",
    "    annotation = annotation.squeeze()\n",
    "    # print(\"After squeeze:\", annotation.shape)\n",
    "\n",
    "    # Unique labels and counts\n",
    "    unique_labels, counts = np.unique(annotation, return_counts=True)\n",
    "\n",
    "    \n",
    "    d = []\n",
    "    for label, count in zip(unique_labels, counts):\n",
    "        if label == 1:\n",
    "            label1 += count\n",
    "        elif label == 2:\n",
    "            label2 += count\n",
    "        elif label == 3:\n",
    "            label3 += count\n",
    "        elif label == 4:\n",
    "            label4 += count\n",
    "        elif label == 5:\n",
    "            label5 += count\n",
    "        elif label == 6:\n",
    "            label6 += count\n",
    "        elif label == 7:\n",
    "            label7 += count\n",
    "        elif label == 8:\n",
    "            label8 += count\n",
    "        elif label == 9:\n",
    "            label9 += count\n",
    "        d.append((label, count))\n",
    "        # print(f\"Label {label}: {count} samples\")\n",
    "    print(f\"File {i:02d} - Label distribution: {d}\")\n",
    "\n",
    "arr = [label1, label2, label3, label4, label5, label6, label7, label8, label9]\n",
    "\n",
    "print(arr)\n",
    "\n",
    "# Plot label over time\n",
    "# plt.figure(figsize=(15,4))\n",
    "# plt.plot(annotation)\n",
    "# plt.title(\"Annotation over time (1 label per second)\")\n",
    "# plt.xlabel(\"Time (seconds)\")\n",
    "# plt.ylabel(\"Label\")\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d2142a23",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [00:08<00:00,  6.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final dataset shape: (36388, 10240)\n",
      "Labels shape: (36388,)\n",
      "Binary label distribution:\n",
      "Label 0: 27899\n",
      "Label 1: 8489\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import scipy.io as sio\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "# -------- SETTINGS --------\n",
    "data_root = \"Preprocessed-Dataset-with-Integrated-Annotations\"\n",
    "window_size = 10      # 10 seconds\n",
    "sampling_rate = 1024  # ECG sampling rate\n",
    "samples_per_window = window_size * sampling_rate\n",
    "\n",
    "X_all = []\n",
    "y_all = []\n",
    "subj_all = []\n",
    "\n",
    "# Loop over all subject folders\n",
    "subject_ids = sorted(os.listdir(data_root))\n",
    "\n",
    "for subj in tqdm(subject_ids):\n",
    "    subj_path = os.path.join(data_root, subj)\n",
    "    \n",
    "    # ECG file name pattern\n",
    "    ecg_file = [f for f in os.listdir(subj_path) if \"ECG\" in f][0]\n",
    "    mat = sio.loadmat(os.path.join(subj_path, ecg_file))\n",
    "    \n",
    "    ecg = mat['ecg_data']         # shape (N_sec, 1, 1024)\n",
    "    annotation = mat['annotation'].squeeze()  # shape (N_sec,)\n",
    "    \n",
    "    # Remove artifact seconds (8, 9)\n",
    "    valid_mask = np.isin(annotation, [0,1,2,3])\n",
    "    ecg = ecg[valid_mask]\n",
    "    annotation = annotation[valid_mask]\n",
    "    \n",
    "    # Convert to binary labels\n",
    "    annotation = np.where(annotation == 0, 0, 1)\n",
    "    \n",
    "    # Remove channel dimension\n",
    "    ecg = ecg.squeeze(1)  # shape (N_sec, 1024)\n",
    "    \n",
    "    # Number of full 10-sec windows\n",
    "    total_seconds = ecg.shape[0]\n",
    "    num_windows = total_seconds // window_size\n",
    "    \n",
    "    for i in range(num_windows):\n",
    "        start = i * window_size\n",
    "        end = start + window_size\n",
    "        \n",
    "        ecg_block = ecg[start:end]  # shape (10, 1024)\n",
    "        label_block = annotation[start:end]\n",
    "        \n",
    "        # Majority label\n",
    "        majority_label = np.bincount(label_block).argmax()\n",
    "        \n",
    "        # Reshape ECG to (10240,)\n",
    "        ecg_block = ecg_block.reshape(-1)\n",
    "        \n",
    "        X_all.append(ecg_block)\n",
    "        y_all.append(majority_label)\n",
    "        subj_all.append(int(subj))  # keep subject ID for LOSO\n",
    "\n",
    "# Convert to numpy arrays\n",
    "X_all = np.array(X_all, dtype=np.float32)\n",
    "y_all = np.array(y_all)\n",
    "subj_all = np.array(subj_all)\n",
    "\n",
    "print(\"Final dataset shape:\", X_all.shape)\n",
    "print(\"Labels shape:\", y_all.shape)\n",
    "\n",
    "# Show distribution\n",
    "unique, counts = np.unique(y_all, return_counts=True)\n",
    "print(\"Binary label distribution:\")\n",
    "for u, c in zip(unique, counts):\n",
    "    print(f\"Label {u}: {c}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "528365ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class ECG_CNN_10s(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ECG_CNN_10s, self).__init__()\n",
    "        \n",
    "        self.conv1 = nn.Conv1d(1, 32, kernel_size=7, stride=1, padding=3)\n",
    "        self.bn1 = nn.BatchNorm1d(32)\n",
    "        self.pool1 = nn.MaxPool1d(4)\n",
    "        \n",
    "        self.conv2 = nn.Conv1d(32, 64, kernel_size=5, stride=1, padding=2)\n",
    "        self.bn2 = nn.BatchNorm1d(64)\n",
    "        self.pool2 = nn.MaxPool1d(4)\n",
    "        \n",
    "        self.conv3 = nn.Conv1d(64, 128, kernel_size=5, stride=1, padding=2)\n",
    "        self.bn3 = nn.BatchNorm1d(128)\n",
    "        self.pool3 = nn.MaxPool1d(4)\n",
    "        \n",
    "        self.conv4 = nn.Conv1d(128, 256, kernel_size=3, stride=1, padding=1)\n",
    "        self.bn4 = nn.BatchNorm1d(256)\n",
    "        \n",
    "        self.global_pool = nn.AdaptiveAvgPool1d(1)\n",
    "        \n",
    "        self.fc1 = nn.Linear(256, 64)\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "        self.fc2 = nn.Linear(64, 2)  # binary classification\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.bn1(self.conv1(x)))\n",
    "        x = self.pool1(x)\n",
    "        \n",
    "        x = F.relu(self.bn2(self.conv2(x)))\n",
    "        x = self.pool2(x)\n",
    "        \n",
    "        x = F.relu(self.bn3(self.conv3(x)))\n",
    "        x = self.pool3(x)\n",
    "        \n",
    "        x = F.relu(self.bn4(self.conv4(x)))\n",
    "        \n",
    "        x = self.global_pool(x)\n",
    "        x = x.squeeze(-1)\n",
    "        \n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc2(x)\n",
    "        \n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1391a9ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "X_tensor = torch.tensor(X_all).unsqueeze(1)  # (N, 1, 10240)\n",
    "y_tensor = torch.tensor(y_all).long()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "86936479",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "import numpy as np\n",
    "\n",
    "class_weights = compute_class_weight(\n",
    "    class_weight='balanced',\n",
    "    classes=np.unique(y_all),\n",
    "    y=y_all\n",
    ")\n",
    "\n",
    "weights = torch.tensor(class_weights, dtype=torch.float32)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss(weight=weights)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7147fab7",
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model = ECG_CNN_10s().to(DEVICE)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bebaa7f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_all,\n",
    "    y_all,\n",
    "    test_size=0.2,\n",
    "    stratify=y_all,\n",
    "    random_state=42\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fec307c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class ECGDataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        X = torch.tensor(X, dtype=torch.float32)\n",
    "        \n",
    "        # Per-sample normalization\n",
    "        mean = X.mean(dim=1, keepdim=True)\n",
    "        std = X.std(dim=1, keepdim=True) + 1e-8\n",
    "        X = (X - mean) / std\n",
    "        \n",
    "        self.X = X.unsqueeze(1)\n",
    "        self.y = torch.tensor(y, dtype=torch.long)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.y[idx]\n",
    "\n",
    "\n",
    "train_dataset = ECGDataset(X_train, y_train)\n",
    "test_dataset  = ECGDataset(X_test, y_test)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "test_loader  = DataLoader(test_dataset, batch_size=32, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6bc71251",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "import numpy as np\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model = ECG_CNN_10s().to(device)\n",
    "\n",
    "# Compute class weights\n",
    "class_weights = compute_class_weight(\n",
    "    class_weight='balanced',\n",
    "    classes=np.unique(y_train),\n",
    "    y=y_train\n",
    ")\n",
    "\n",
    "weights = torch.tensor(class_weights, dtype=torch.float32).to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss(weight=weights)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "84ca8950",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "Train Loss: 0.5978\n",
      "Test Acc: 0.7667 | F1: 0.0000 | AUC: 0.5191\n",
      "Epoch 2\n",
      "Train Loss: 0.5283\n",
      "Test Acc: 0.7667 | F1: 0.0000 | AUC: 0.5525\n",
      "Epoch 3\n",
      "Train Loss: 0.5081\n",
      "Test Acc: 0.7667 | F1: 0.0000 | AUC: 0.5930\n",
      "Epoch 4\n",
      "Train Loss: 0.4946\n",
      "Test Acc: 0.2333 | F1: 0.3783 | AUC: 0.5013\n",
      "Epoch 5\n",
      "Train Loss: 0.4874\n",
      "Test Acc: 0.2333 | F1: 0.3782 | AUC: 0.5381\n",
      "Epoch 6\n",
      "Train Loss: 0.4803\n",
      "Test Acc: 0.7667 | F1: 0.0000 | AUC: 0.5456\n",
      "Epoch 7\n",
      "Train Loss: 0.4752\n",
      "Test Acc: 0.2332 | F1: 0.3780 | AUC: 0.4996\n",
      "Epoch 8\n",
      "Train Loss: 0.4649\n",
      "Test Acc: 0.2332 | F1: 0.3780 | AUC: 0.4921\n",
      "Early stopping triggered.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, f1_score, roc_auc_score\n",
    "import numpy as np\n",
    "\n",
    "num_epochs = 30\n",
    "best_auc = 0\n",
    "patience = 5\n",
    "patience_counter = 0\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    \n",
    "    # ---- TRAIN ----\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    \n",
    "    for X_batch, y_batch in train_loader:\n",
    "        X_batch = X_batch.to(device)\n",
    "        y_batch = y_batch.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(X_batch)\n",
    "        loss = criterion(outputs, y_batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        train_loss += loss.item()\n",
    "    \n",
    "    # ---- EVAL ----\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    all_probs = []\n",
    "    all_labels = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for X_batch, y_batch in test_loader:\n",
    "            X_batch = X_batch.to(device)\n",
    "            y_batch = y_batch.to(device)\n",
    "            \n",
    "            outputs = model(X_batch)\n",
    "            probs = torch.softmax(outputs, dim=1)[:, 1]\n",
    "            preds = torch.argmax(outputs, dim=1)\n",
    "            \n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_probs.extend(probs.cpu().numpy())\n",
    "            all_labels.extend(y_batch.cpu().numpy())\n",
    "    \n",
    "    acc = accuracy_score(all_labels, all_preds)\n",
    "    f1 = f1_score(all_labels, all_preds)\n",
    "    auc = roc_auc_score(all_labels, all_probs)\n",
    "    \n",
    "    print(f\"Epoch {epoch+1}\")\n",
    "    print(f\"Train Loss: {train_loss/len(train_loader):.4f}\")\n",
    "    print(f\"Test Acc: {acc:.4f} | F1: {f1:.4f} | AUC: {auc:.4f}\")\n",
    "    \n",
    "    # ---- Early Stopping ----\n",
    "    if auc > best_auc:\n",
    "        best_auc = auc\n",
    "        torch.save(model.state_dict(), \"best_ecg_model.pth\")\n",
    "        patience_counter = 0\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "    \n",
    "    if patience_counter >= patience:\n",
    "        print(\"Early stopping triggered.\")\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "453c733a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "import numpy as np\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model = ECG_CNN_10s().to(device)\n",
    "\n",
    "# Compute class weights\n",
    "class_weights = compute_class_weight(\n",
    "    class_weight='balanced',\n",
    "    classes=np.unique(y_train),\n",
    "    y=y_train\n",
    ")\n",
    "\n",
    "weights = torch.tensor(class_weights, dtype=torch.float32).to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss(weight=weights)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=5e-4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1659ea55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "Train Loss: 0.5730\n",
      "Test Acc: 0.6708 | F1: 0.5447 | AUC: 0.8276\n",
      "Epoch 2\n",
      "Train Loss: 0.5183\n",
      "Test Acc: 0.7770 | F1: 0.5996 | AUC: 0.8416\n",
      "Epoch 3\n",
      "Train Loss: 0.4996\n",
      "Test Acc: 0.8306 | F1: 0.6179 | AUC: 0.8371\n",
      "Epoch 4\n",
      "Train Loss: 0.4914\n",
      "Test Acc: 0.7872 | F1: 0.6136 | AUC: 0.8587\n",
      "Epoch 5\n",
      "Train Loss: 0.4785\n",
      "Test Acc: 0.8190 | F1: 0.6188 | AUC: 0.8608\n",
      "Epoch 6\n",
      "Train Loss: 0.4683\n",
      "Test Acc: 0.8097 | F1: 0.6202 | AUC: 0.8538\n",
      "Epoch 7\n",
      "Train Loss: 0.4672\n",
      "Test Acc: 0.6832 | F1: 0.5656 | AUC: 0.8594\n",
      "Epoch 8\n",
      "Train Loss: 0.4608\n",
      "Test Acc: 0.7579 | F1: 0.6060 | AUC: 0.8683\n",
      "Epoch 9\n",
      "Train Loss: 0.4583\n",
      "Test Acc: 0.7458 | F1: 0.5982 | AUC: 0.8631\n",
      "Epoch 10\n",
      "Train Loss: 0.4519\n",
      "Test Acc: 0.7270 | F1: 0.5988 | AUC: 0.8770\n",
      "Epoch 11\n",
      "Train Loss: 0.4504\n",
      "Test Acc: 0.8435 | F1: 0.6374 | AUC: 0.8828\n",
      "Epoch 12\n",
      "Train Loss: 0.4462\n",
      "Test Acc: 0.8182 | F1: 0.6356 | AUC: 0.8761\n",
      "Epoch 13\n",
      "Train Loss: 0.4424\n",
      "Test Acc: 0.8226 | F1: 0.6468 | AUC: 0.8825\n",
      "Epoch 14\n",
      "Train Loss: 0.4377\n",
      "Test Acc: 0.7205 | F1: 0.5958 | AUC: 0.8750\n",
      "Epoch 15\n",
      "Train Loss: 0.4328\n",
      "Test Acc: 0.7815 | F1: 0.6287 | AUC: 0.8827\n",
      "Epoch 16\n",
      "Train Loss: 0.4336\n",
      "Test Acc: 0.7792 | F1: 0.6308 | AUC: 0.8848\n",
      "Epoch 17\n",
      "Train Loss: 0.4274\n",
      "Test Acc: 0.8045 | F1: 0.6378 | AUC: 0.8829\n",
      "Epoch 18\n",
      "Train Loss: 0.4264\n",
      "Test Acc: 0.7340 | F1: 0.6070 | AUC: 0.8793\n",
      "Epoch 19\n",
      "Train Loss: 0.4267\n",
      "Test Acc: 0.6658 | F1: 0.5660 | AUC: 0.8760\n",
      "Epoch 20\n",
      "Train Loss: 0.4196\n",
      "Test Acc: 0.8149 | F1: 0.6458 | AUC: 0.8836\n",
      "Epoch 21\n",
      "Train Loss: 0.4148\n",
      "Test Acc: 0.6998 | F1: 0.5839 | AUC: 0.8761\n",
      "Early stopping triggered.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, f1_score, roc_auc_score\n",
    "import numpy as np\n",
    "\n",
    "num_epochs = 30\n",
    "best_auc = 0\n",
    "patience = 5\n",
    "patience_counter = 0\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    \n",
    "    # ---- TRAIN ----\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    \n",
    "    for X_batch, y_batch in train_loader:\n",
    "        X_batch = X_batch.to(device)\n",
    "        y_batch = y_batch.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(X_batch)\n",
    "        loss = criterion(outputs, y_batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        train_loss += loss.item()\n",
    "    \n",
    "    # ---- EVAL ----\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    all_probs = []\n",
    "    all_labels = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for X_batch, y_batch in test_loader:\n",
    "            X_batch = X_batch.to(device)\n",
    "            y_batch = y_batch.to(device)\n",
    "            \n",
    "            outputs = model(X_batch)\n",
    "            probs = torch.softmax(outputs, dim=1)[:, 1]\n",
    "            preds = torch.argmax(outputs, dim=1)\n",
    "            \n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_probs.extend(probs.cpu().numpy())\n",
    "            all_labels.extend(y_batch.cpu().numpy())\n",
    "    \n",
    "    acc = accuracy_score(all_labels, all_preds)\n",
    "    f1 = f1_score(all_labels, all_preds)\n",
    "    auc = roc_auc_score(all_labels, all_probs)\n",
    "    \n",
    "    print(f\"Epoch {epoch+1}\")\n",
    "    print(f\"Train Loss: {train_loss/len(train_loader):.4f}\")\n",
    "    print(f\"Test Acc: {acc:.4f} | F1: {f1:.4f} | AUC: {auc:.4f}\")\n",
    "    \n",
    "    # ---- Early Stopping ----\n",
    "    if auc > best_auc:\n",
    "        best_auc = auc\n",
    "        torch.save(model.state_dict(), \"best_ecg_model2.pth\")\n",
    "        patience_counter = 0\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "    \n",
    "    if patience_counter >= patience:\n",
    "        print(\"Early stopping triggered.\")\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30ceb7eb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "opendriver",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
